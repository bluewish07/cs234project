\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{multigames}
\citation{multirobot}
\citation{communication}
\citation{selfplay}
\citation{unstable}
\citation{maddpg}
\citation{sandholm1996}
\citation{claus1998dynamics}
\citation{cooperativeQ}
\citation{buffet2007}
\citation{busoniu2008comprehensive}
\citation{bucsoniu2010multi}
\newlabel{submission}{{1}{1}{}{section.1}{}}
\citation{boutilier1996planning}
\citation{chalkiadakis2003coordination}
\citation{maddpg}
\citation{suttonbarto1998rl}
\citation{lillicrap2015continuous}
\citation{coma}
\citation{openaigym}
\citation{maddpg}
\newlabel{fig:diagram}{{2}{3}{MADDPG algorithm with centralized Q functions. As indicating by the dotted lines, note that the Q functions take the entire multi-agent state as an input in addition to the actions of all agents}{figure.2}{}}
\citation{maddpg}
<<<<<<< HEAD
\newlabel{tab:basecase_parameters}{{4.2}{4}{}{subsection.4.2}{}}
\newlabel{fig:MADDPGvsPG}{{3}{4}{Average reward achieved by policy gradient algorithm and MADDPG algorithm}{figure.3}{}}
\newlabel{fig:MADDPGvsPG_distance}{{4}{5}{Average distance from landmarks with Naive PG algorithm and MADDPG algorithm}{figure.4}{}}
\newlabel{fig:defaultVSouVSdist}{{5}{5}{Average reward gained by using different exploration strategies}{figure.5}{}}
\newlabel{fig:action_exploration_range}{{6}{5}{Average reward gained by using different exploration strategies}{figure.6}{}}
\newlabel{fig:action_exploration_std_dev}{{7}{5}{Average reward gained by using different exploration strategies}{figure.7}{}}
=======
\citation{plappert2017parameter}
\newlabel{fig:MADDPGvsPG}{{2}{4}{Average reward achieved by Naive PG (REINFORCE) algorithm and MADDPG algorithm}{figure.2}{}}
\newlabel{tab:basecase_parameters}{{1}{4}{Parameters used for training MADDPG}{table.1}{}}
\newlabel{fig:MADDPGvsPG_distance}{{3}{4}{Average distance from landmarks with Naive PG algorithm and MADDPG algorithm}{figure.3}{}}
\newlabel{fig:defaultVSouVSdist}{{4}{4}{Average reward gained by using different exploration strategies}{figure.4}{}}
\newlabel{fig:action_exploration_range}{{5}{5}{Range of effective euclidean distance for different exploration strategies}{figure.5}{}}
\newlabel{fig:action_exploration_std_dev}{{6}{5}{Standard deviation of effective euclidean distance for different exploration strategies}{figure.6}{}}
\newlabel{fig:avg_reward_multiple_agents}{{7}{5}{Average reward gained by each agent in environment with different number of agents and landmarks}{figure.7}{}}
>>>>>>> 41b64b93046ed41fe6f4d2d1c91fdc819f311148
\bibdata{final}
\bibcite{boutilier1996planning}{{1}{1996}{{Boutilier}}{{}}}
\newlabel{fig:avg_reward_multiple_agents}{{8}{6}{Average reward gained by using different exploration strategies}{figure.8}{}}
\bibcite{buffet2007}{{2}{2007}{{Buffet et~al.}}{{Buffet, Dutech, and Charpillet}}}
\bibcite{busoniu2008comprehensive}{{3}{2008}{{Busoniu et~al.}}{{Busoniu, Babuska, and De~Schutter}}}
\bibcite{bucsoniu2010multi}{{4}{2010}{{Bu{\c {s}}oniu et~al.}}{{Bu{\c {s}}oniu, Babu{\v {s}}ka, and De~Schutter}}}
\bibcite{chalkiadakis2003coordination}{{5}{2003}{{Chalkiadakis \& Boutilier}}{{Chalkiadakis and Boutilier}}}
\bibcite{claus1998dynamics}{{6}{1998}{{Claus \& Boutilier}}{{Claus and Boutilier}}}
\bibcite{communication}{{7}{2016}{{Foerster et~al.}}{{Foerster, Assael, de~Freitas, and Whiteson}}}
<<<<<<< HEAD
\bibcite{coma}{{8}{2017}{{Foerster et~al.}}{{Foerster, Farquhar, Afouras, Nardelli, and Whiteson}}}
\bibcite{cooperativeQ}{{9}{2000}{{Lauer \& Riedmiller}}{{Lauer and Riedmiller}}}
\bibcite{lillicrap2015continuous}{{10}{2015}{{Lillicrap et~al.}}{{Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}}}
\bibcite{maddpg}{{11}{2017}{{Lowe et~al.}}{{Lowe, Wu, Tamar, Harb, Abbeel, and Mordatch}}}
\bibcite{multirobot}{{12}{2012{a}}{{Matignon et~al.}}{{Matignon, Jeanpierre, and et~al.}}}
\bibcite{unstable}{{13}{2012{b}}{{Matignon et~al.}}{{Matignon, Laurent, and Fort-Piat}}}
\bibcite{openaigym}{{14}{2017}{{OpenAI}}{{}}}
\bibcite{multigames}{{15}{2017}{{Peng et~al.}}{{Peng, Yuan, Wen, Yang, Tang, Long, and Wang}}}
=======
\bibcite{lillicrap2015continuous}{{8}{2015}{{Lillicrap et~al.}}{{Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}}}
\bibcite{maddpg}{{9}{2017}{{Lowe et~al.}}{{Lowe, Wu, Tamar, Harb, Abbeel, and Mordatch}}}
\bibcite{multirobot}{{10}{2012{a}}{{Matignon et~al.}}{{Matignon, Jeanpierre, and et~al.}}}
\bibcite{unstable}{{11}{2012{b}}{{Matignon et~al.}}{{Matignon, Laurent, and Fort-Piat}}}
\bibcite{openaigym}{{12}{2017}{{OpenAI}}{{}}}
\bibcite{panait2005cooperative}{{13}{2005}{{Panait \& Luke}}{{Panait and Luke}}}
\bibcite{multigames}{{14}{2017}{{Peng et~al.}}{{Peng, Yuan, Wen, Yang, Tang, Long, and Wang}}}
\bibcite{plappert2017parameter}{{15}{2017}{{Plappert et~al.}}{{Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen, Asfour, Abbeel, and Andrychowicz}}}
>>>>>>> 41b64b93046ed41fe6f4d2d1c91fdc819f311148
\bibcite{sandholm1996}{{16}{1996}{{Sandholm \& Crites}}{{Sandholm and Crites}}}
\bibcite{selfplay}{{17}{2017}{{Sukhbaatar et~al.}}{{Sukhbaatar, Kostrikov, Szlam, , and Fergus}}}
\bibcite{suttonbarto1998rl}{{18}{1998}{{Sutton \& Barto}}{{Sutton and Barto}}}
\bibstyle{icml2018}
